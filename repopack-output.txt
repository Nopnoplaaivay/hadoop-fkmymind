This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-08-20T10:20:52.877Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
cleanup.sh
config/all-nodes/mapred-site.xml
config/datanode/core-site.xml
config/datanode/hdfs-site.xml
config/namenode/core-site.xml
config/namenode/hdfs-site.xml
config/nodemanager/yarn-site.xml
config/resourcemanager/capacity-scheduler.xml
config/resourcemanager/yarn-site.xml
docker-compose.yml
Dockerfile.multi
readme.md
repo-multinode.txt
scripts/start-datanode.sh
scripts/start-namenode.sh
scripts/start-nodemanager.sh
scripts/start-resourcemanager.sh
setup-multi-node.sh
single_node/build.md
single_node/check-hadoop.sh
single_node/config/core-site.xml
single_node/config/hdfs-site.xml
single_node/config/mapred-site.xml
single_node/config/yarn-site.xml
single_node/Dockerfile
single_node/start-hadoop.sh
stop-cluster.sh

================================================================
Repository Files
================================================================

================
File: .gitignore
================
# IDE
.idea
# venv
venv

# config
*.env
*.ini

# cpython
*__pycache__*

# other
tmp

# data
*.db

# notebooks
notebooks

================
File: cleanup.sh
================
echo "Cleaning up Hadoop cluster..."
docker-compose down -v
docker rmi hadoop-multi:latest
rm -rf data/*
echo "Cleanup complete."

================
File: config/all-nodes/mapred-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
</configuration>

================
File: config/datanode/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
</configuration>

================
File: config/datanode/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hdfs/datanode</value>`
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>

================
File: config/namenode/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
</configuration>

================
File: config/namenode/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>

================
File: config/nodemanager/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>
</configuration>

================
File: config/resourcemanager/capacity-scheduler.xml
================
<?xml version="1.0"?>
<configuration>
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>100</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>100</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.maximum-applications</name>
    <value>10000</value>
  </property>
</configuration>

================
File: config/resourcemanager/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>resourcemanager:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>resourcemanager:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>resourcemanager:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>resourcemanager:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>resourcemanager:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

================
File: docker-compose.yml
================
version: '3.8'

services:
  namenode:
    image: hadoop-multi:latest
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"
      # - "9000:9000"
      - "50070:50070"
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - NODE_TYPE=namenode
    volumes:
      - ./config/namenode:/home/hadoop/hadoop/etc/hadoop
      - namenode_data:/home/hadoop/hdfs/namenode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-namenode.sh

  datanode1:
    image: hadoop-multi:latest
    container_name: datanode1
    hostname: datanode1
    depends_on:
      - namenode
    ports:
      - "9864:9864"
      - "50075:50075"
    environment:
      - NODE_TYPE=datanode
      - NAMENODE_HOST=namenode
    volumes:
      - ./config/datanode:/home/hadoop/hadoop/etc/hadoop
      - datanode1_data:/home/hadoop/hdfs/datanode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-datanode.sh

  datanode2:
    image: hadoop-multi:latest
    container_name: datanode2
    hostname: datanode2
    depends_on:
      - namenode
    ports:
      - "9865:9864"
      - "50076:50075"
    environment:
      - NODE_TYPE=datanode
      - NAMENODE_HOST=namenode
    volumes:
      - ./config/datanode:/home/hadoop/hadoop/etc/hadoop
      - datanode2_data:/home/hadoop/hdfs/datanode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-datanode.sh

  resourcemanager:
    image: hadoop-multi:latest
    container_name: resourcemanager
    hostname: resourcemanager
    depends_on:
      - namenode
    ports:
      - "8088:8088"
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
      - "8033:8033"
    environment:
      - NODE_TYPE=resourcemanager
    volumes:
      - ./config/resourcemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: ["/bin/bash","-lc","sed -i 's/\\r$//' /home/hadoop/scripts/*.sh && chmod +x /home/hadoop/scripts/*.sh && exec /home/hadoop/scripts/start-resourcemanager.sh"]

  nodemanager1:
    image: hadoop-multi:latest
    container_name: nodemanager1
    hostname: nodemanager1
    depends_on:
      - resourcemanager
      - datanode1
    ports:
      - "8042:8042"
    environment:
      - NODE_TYPE=nodemanager
      - RESOURCEMANAGER_HOST=resourcemanager
    volumes:
      - ./config/nodemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: ["/bin/bash","-lc","sed -i 's/\\r$//' /home/hadoop/scripts/*.sh && chmod +x /home/hadoop/scripts/*.sh && exec /home/hadoop/scripts/start-nodemanager.sh"]

  nodemanager2:
    image: hadoop-multi:latest
    container_name: nodemanager2
    hostname: nodemanager2
    depends_on:
      - resourcemanager
      - datanode2
    ports:
      - "8043:8042"
    environment:
      - NODE_TYPE=nodemanager
      - RESOURCEMANAGER_HOST=resourcemanager
    volumes:
      - ./config/nodemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: ["/bin/bash","-lc","sed -i 's/\\r$//' /home/hadoop/scripts/*.sh && chmod +x /home/hadoop/scripts/*.sh && exec /home/hadoop/scripts/start-nodemanager.sh"]

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:

networks:
  hadoop-network:
    driver: bridge

================
File: Dockerfile.multi
================
FROM ubuntu:20.04

ENV DEBIAN_FRONTEND=noninteractive

# Install dependencies
RUN apt-get update && apt-get install -y \
    openjdk-8-jdk \
    ssh \
    rsync \
    vim \
    net-tools \
    curl \
    wget \
    sudo \
    netcat \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Create hadoop user
RUN useradd -m -s /bin/bash hadoop && \
    echo "hadoop:hadoop" | chpasswd && \
    adduser hadoop sudo

RUN printf "hadoop ALL=(ALL) NOPASSWD:ALL\n" > /etc/sudoers.d/010-hadoop-nopasswd && \
    chmod 440 /etc/sudoers.d/010-hadoop-nopasswd


# Configure SSH
RUN ssh-keygen -A && \
    echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "PasswordAuthentication yes" >> /etc/ssh/sshd_config && \
    echo "PubkeyAuthentication yes" >> /etc/ssh/sshd_config

USER hadoop
WORKDIR /home/hadoop

# Download and install Hadoop
RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

# Set Hadoop environment
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV YARN_HOME=$HADOOP_HOME
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Setup SSH keys
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "   StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "   UserKnownHostsFile /dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# Create necessary directories
RUN mkdir -p /home/hadoop/hdfs/namenode && \
    mkdir -p /home/hadoop/hdfs/datanode && \
    mkdir -p /home/hadoop/tmp && \
    mkdir -p /home/hadoop/logs

USER root

# Make scripts directory
RUN mkdir -p /home/hadoop/scripts && \
    chown -R hadoop:hadoop /home/hadoop

EXPOSE 9870 9864 8088 9000 50070 50075 50010 50020 50090 8030 8031 8032 8033 8040 8042 49707 2122

USER hadoop

CMD ["/bin/bash"]

================
File: readme.md
================
docker build -t hadoop-multi:latest -f Dockerfile.multi .
docker-compose up -d
docker exec namenode hdfs dfsadmin -report

================
File: repo-multinode.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-08-19T18:47:08.753Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
cleanup.sh
config/all-nodes/mapred-site.xml
config/datanode/core-site.xml
config/datanode/hdfs-site.xml
config/namenode/core-site.xml
config/namenode/hdfs-site.xml
config/nodemanager/yarn-site.xml
config/resourcemanager/yarn-site.xml
docker-compose.yml
Dockerfile.multi
scripts/start-datanode.sh
scripts/start-namenode.sh
scripts/start-nodemanager.sh
scripts/start-resourcemanager.sh
setup-multi-node.sh
single_node/build.md
single_node/check-hadoop.sh
single_node/config/core-site.xml
single_node/config/hdfs-site.xml
single_node/config/mapred-site.xml
single_node/config/yarn-site.xml
single_node/Dockerfile
single_node/start-hadoop.sh
stop-cluster.sh

================================================================
Repository Files
================================================================

================
File: .gitignore
================
# IDE
.idea
# venv
venv

# config
*.env
*.ini

# cpython
*__pycache__*

# other
tmp

# data
*.db

# notebooks
notebooks

================
File: cleanup.sh
================
echo "Cleaning up Hadoop cluster..."
docker-compose down -v
docker rmi hadoop-multi:latest
rm -rf data/*
echo "Cleanup complete."

================
File: config/all-nodes/mapred-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
    </property>
</configuration>

================
File: config/datanode/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
</configuration>

================
File: config/datanode/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>

================
File: config/namenode/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
</configuration>

================
File: config/namenode/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hdfs/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>

================
File: config/nodemanager/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>
</configuration>

================
File: config/resourcemanager/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>resourcemanager</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>resourcemanager:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>resourcemanager:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>resourcemanager:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>resourcemanager:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>resourcemanager:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

================
File: docker-compose.yml
================
version: '3.8'

services:
  namenode:
    image: hadoop-multi:latest
    container_name: namenode
    hostname: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
      - "50070:50070"
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - NODE_TYPE=namenode
    volumes:
      - ./config/namenode:/home/hadoop/hadoop/etc/hadoop
      - namenode_data:/home/hadoop/hdfs/namenode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-namenode.sh

  datanode1:
    image: hadoop-multi:latest
    container_name: datanode1
    hostname: datanode1
    depends_on:
      - namenode
    ports:
      - "9864:9864"
      - "50075:50075"
    environment:
      - NODE_TYPE=datanode
      - NAMENODE_HOST=namenode
    volumes:
      - ./config/datanode:/home/hadoop/hadoop/etc/hadoop
      - datanode1_data:/home/hadoop/hdfs/datanode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-datanode.sh

  datanode2:
    image: hadoop-multi:latest
    container_name: datanode2
    hostname: datanode2
    depends_on:
      - namenode
    ports:
      - "9865:9864"
      - "50076:50075"
    environment:
      - NODE_TYPE=datanode
      - NAMENODE_HOST=namenode
    volumes:
      - ./config/datanode:/home/hadoop/hadoop/etc/hadoop
      - datanode2_data:/home/hadoop/hdfs/datanode
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-datanode.sh

  resourcemanager:
    image: hadoop-multi:latest
    container_name: resourcemanager
    hostname: resourcemanager
    depends_on:
      - namenode
    ports:
      - "8088:8088"
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
      - "8033:8033"
    environment:
      - NODE_TYPE=resourcemanager
    volumes:
      - ./config/resourcemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-resourcemanager.sh

  nodemanager1:
    image: hadoop-multi:latest
    container_name: nodemanager1
    hostname: nodemanager1
    depends_on:
      - resourcemanager
      - datanode1
    ports:
      - "8042:8042"
    environment:
      - NODE_TYPE=nodemanager
      - RESOURCEMANAGER_HOST=resourcemanager
    volumes:
      - ./config/nodemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-nodemanager.sh

  nodemanager2:
    image: hadoop-multi:latest
    container_name: nodemanager2
    hostname: nodemanager2
    depends_on:
      - resourcemanager
      - datanode2
    ports:
      - "8043:8042"
    environment:
      - NODE_TYPE=nodemanager
      - RESOURCEMANAGER_HOST=resourcemanager
    volumes:
      - ./config/nodemanager:/home/hadoop/hadoop/etc/hadoop
      - ./scripts:/home/hadoop/scripts
    networks:
      - hadoop-network
    command: /home/hadoop/scripts/start-nodemanager.sh

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:

networks:
  hadoop-network:
    driver: bridge

================
File: Dockerfile.multi
================
FROM ubuntu:20.04

ENV DEBIAN_FRONTEND=noninteractive

# Install dependencies
RUN apt-get update && apt-get install -y \
    openjdk-8-jdk \
    ssh \
    rsync \
    vim \
    net-tools \
    curl \
    wget \
    sudo \
    netcat \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Create hadoop user
RUN useradd -m -s /bin/bash hadoop && \
    echo "hadoop:hadoop" | chpasswd && \
    adduser hadoop sudo

# Configure SSH
RUN ssh-keygen -A && \
    echo "PermitRootLogin yes" >> /etc/ssh/sshd_config && \
    echo "PasswordAuthentication yes" >> /etc/ssh/sshd_config && \
    echo "PubkeyAuthentication yes" >> /etc/ssh/sshd_config

USER hadoop
WORKDIR /home/hadoop

# Download and install Hadoop
RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

# Set Hadoop environment
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV HADOOP_MAPRED_HOME=$HADOOP_HOME
ENV HADOOP_COMMON_HOME=$HADOOP_HOME
ENV HADOOP_HDFS_HOME=$HADOOP_HOME
ENV YARN_HOME=$HADOOP_HOME
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# Setup SSH keys
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "   StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "   UserKnownHostsFile /dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# Create necessary directories
RUN mkdir -p /home/hadoop/hdfs/namenode && \
    mkdir -p /home/hadoop/hdfs/datanode && \
    mkdir -p /home/hadoop/tmp && \
    mkdir -p /home/hadoop/logs

USER root

# Make scripts directory
RUN mkdir -p /home/hadoop/scripts && \
    chown -R hadoop:hadoop /home/hadoop

EXPOSE 9870 9864 8088 9000 50070 50075 50010 50020 50090 8030 8031 8032 8033 8040 8042 49707 2122

USER hadoop

CMD ["/bin/bash"]

================
File: scripts/start-datanode.sh
================
sudo service ssh start

# Wait for SSH and NameNode to be ready
sleep 10

# Wait for namenode to be available
while ! nc -z namenode 9000; do
    echo "Waiting for NameNode to be available..."
    sleep 2
done

echo "Starting DataNode..."
hdfs datanode

================
File: scripts/start-namenode.sh
================
#!/bin/bash
sudo service ssh start

# Wait for SSH to start
sleep 5

# Format namenode if not already formatted
if [ ! -d "/home/hadoop/hdfs/namenode/current" ]; then
    echo "Formatting namenode..."
    hdfs namenode -format -force
fi

echo "Starting NameNode..."
hdfs namenode

================
File: scripts/start-nodemanager.sh
================
#!/bin/bash
sudo service ssh start

# Wait for SSH to start
sleep 5

# Wait for ResourceManager to be available
while ! nc -z resourcemanager 8032; do
    echo "Waiting for ResourceManager..."
    sleep 2
done

echo "Starting NodeManager..."
yarn nodemanager

================
File: scripts/start-resourcemanager.sh
================
#!/bin/bash
sudo service ssh start

# Wait for SSH to start
sleep 5

# Wait for namenode to be available
while ! nc -z namenode 9000; do
    echo "Waiting for NameNode..."
    sleep 2
done

echo "Starting ResourceManager..."
yarn resourcemanager

================
File: setup-multi-node.sh
================
#!/bin/bash

echo "Setting up Hadoop Multi-Node Cluster..."

# Create directory structure
mkdir -p config/{namenode,datanode,resourcemanager,nodemanager}
mkdir -p scripts
mkdir -p data

# Make scripts executable
chmod +x scripts/*.sh

# Build Docker image
echo "Building Hadoop Multi-Node Docker image..."
docker build -t hadoop-multi:latest -f Dockerfile.multi .

# Start the cluster
echo "Starting Hadoop cluster..."
docker-compose up -d

# Wait for services to start
echo "Waiting for services to initialize..."
sleep 30

# Check cluster status
echo "Checking cluster status..."
docker exec namenode hdfs dfsadmin -report

echo "================================================"
echo "Hadoop Multi-Node Cluster is running!"
echo "NameNode UI: http://localhost:9870"
echo "ResourceManager UI: http://localhost:8088"
echo "================================================"

================
File: single_node/build.md
================
# Hadoop Docker Setup 

docker build -t hadoop-single .
docker run -d --name hadoop-single-node -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname localhost hadoop-single
docker exec -it hadoop-single-node bash


- **HDFS NameNode Web UI**: http://localhost:9870
- **YARN ResourceManager Web UI**: http://localhost:8088

================
File: single_node/check-hadoop.sh
================
#!/bin/bash
echo "=== Hadoop Services Health Check ==="
echo

# Check if SSH is running
echo "1. SSH Service Status:"
sudo service ssh status | grep -E "(Active|inactive)"
echo

# Check environment variables
echo "2. Environment Variables:"
echo "JAVA_HOME: $JAVA_HOME"
echo "HADOOP_HOME: $HADOOP_HOME"
echo "PATH contains Hadoop: $(echo $PATH | grep -o 'hadoop' | head -1)"
echo

# Check Java version
echo "3. Java Version:"
java -version 2>&1 | head -1
echo

# Check running Hadoop processes
echo "4. Running Hadoop Processes:"
jps
echo

# Check HDFS status
echo "5. HDFS Health:"
if command -v hdfs &> /dev/null; then
    $HADOOP_HOME/bin/hdfs dfsadmin -report | head -10
else
    echo "HDFS command not found - check HADOOP_HOME"
fi
echo

# Check web UIs accessibility
echo "6. Web UI Status:"
echo "NameNode Web UI: http://localhost:9870"
echo "ResourceManager Web UI: http://localhost:8088"
echo

echo "=== Health Check Complete ==="

================
File: single_node/config/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hadoop/tmp</value>
    </property>
</configuration>

================
File: single_node/config/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/hadoop/data/datanode</value>
    </property>
</configuration>

================
File: single_node/config/mapred-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

================
File: single_node/config/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

================
File: single_node/Dockerfile
================
FROM ubuntu:20.04

# Cài đặt các dependencies
RUN apt-get update && \
    apt-get install -y openjdk-8-jdk wget ssh openssh-server openssh-client rsync sudo && \
    apt-get clean

# Tạo user hadoop
RUN useradd -m -s /bin/bash hadoop && \
    echo 'hadoop:hadoop' | chpasswd && \
    adduser hadoop sudo

# Cài đặt Hadoop
USER hadoop
WORKDIR /home/hadoop

RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

# Thiết lập biến môi trường
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# Thêm environment variables vào bash profile để SSH session có thể sử dụng
RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_HOME=/home/hadoop/hadoop' >> /home/hadoop/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> /home/hadoop/.bashrc

# Copy các file cấu hình và startup script
USER root
COPY config/* /home/hadoop/hadoop/etc/hadoop/
COPY start-hadoop.sh /home/hadoop/
COPY check-hadoop.sh /home/hadoop/

# cấp quyền chạy và đổi quyền chủ sở hữu
RUN chown -R hadoop:hadoop /home/hadoop/hadoop && \
    # chmod +x /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh && \
    chmod +x /home/hadoop/start-hadoop.sh && \
    chmod +x /home/hadoop/check-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/start-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/check-hadoop.sh

# SSH configuration for root
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# SSH configuration for hadoop user
USER hadoop
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# Expose ports
EXPOSE 9870 8088 9000

# Switch to hadoop user and set working directory
USER hadoop
WORKDIR /home/hadoop

CMD ["./start-hadoop.sh"]

================
File: single_node/start-hadoop.sh
================
#!/bin/bash

# Source the Hadoop environment
# source $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Ensure environment variables are set
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export YARN_LOG_DIR=$HADOOP_HOME/logs


export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"


# Format the namenode (only if not already formatted)
if [ ! -d "/home/hadoop/hadoop/logs/hadoop" ]; then
    echo "Formatting namenode..."
    $HADOOP_HOME/bin/hdfs namenode -format -force
fi


$HADOOP_HOME/bin/hdfs --daemon start namenode
$HADOOP_HOME/bin/hdfs --daemon start datanode
$HADOOP_HOME/bin/yarn --daemon start resourcemanager
$HADOOP_HOME/bin/yarn --daemon start nodemanager

# Start Hadoop services
# echo "Starting HDFS services..."
# $HADOOP_HOME/sbin/start-dfs.sh

# echo "Starting YARN services..."
# $HADOOP_HOME/sbin/start-yarn.sh

# Show running services
echo "Hadoop services status:"
jps

# Keep container running
echo "All Hadoop services started successfully!"
echo "HDFS NameNode Web UI: http://localhost:9870"
echo "YARN ResourceManager Web UI: http://localhost:8088"
echo "Keeping container alive..."
tail -f /dev/null

================
File: stop-cluster.sh
================
#!/bin/bash
echo "Stopping Hadoop cluster..."
docker-compose down
echo "Cluster stopped."

================
File: scripts/start-datanode.sh
================
#!/bin/bash
sudo service ssh start

# Wait for SSH and NameNode to be ready
sleep 10

# Wait for namenode to be available
while ! nc -z namenode 9000; do
    echo "Waiting for NameNode to be available..."
    sleep 2
done

echo "Starting DataNode..."
hdfs datanode

================
File: scripts/start-namenode.sh
================
#!/bin/bash
sudo service ssh start

# Wait for SSH to start
sleep 5

# Format namenode if not already formatted
if [ ! -d "/home/hadoop/hdfs/namenode/current" ]; then
    echo "Formatting namenode..."
    hdfs namenode -format -force
fi

echo "Starting NameNode..."
hdfs namenode

================
File: scripts/start-nodemanager.sh
================
#!/bin/bash
set -euo pipefail
sudo service ssh start


# Wait for ResourceManager to be available
while ! nc -z resourcemanager 8032; do
    echo "Waiting for ResourceManager..."
    sleep 2
done

echo "Starting NodeManager..."
exec yarn nodemanager

================
File: scripts/start-resourcemanager.sh
================
#!/bin/bash
set -euo pipefail
sudo -n service ssh start

# Đợi NameNode sẵn sàng
while ! nc -z namenode 9000; do
  echo "Waiting for NameNode..."
  sleep 2
done

echo "Starting ResourceManager..."
exec yarn resourcemanager

================
File: setup-multi-node.sh
================
#!/bin/bash
echo "Setting up Hadoop Multi-Node Cluster..."

# Create directory structure
mkdir -p config/{namenode,datanode,resourcemanager,nodemanager}
mkdir -p scripts
mkdir -p data

# Make scripts executable
chmod +x scripts/*.sh

# Build Docker image
echo "Building Hadoop Multi-Node Docker image..."
docker build -t hadoop-multi:latest -f Dockerfile.multi .

# Start the cluster
echo "Starting Hadoop cluster..."
docker-compose up -d

# Wait for services to start
echo "Waiting for services to initialize..."
sleep 30

# Check cluster status
echo "Checking cluster status..."
docker exec namenode hdfs dfsadmin -report

echo "================================================"
echo "Hadoop Multi-Node Cluster is running!"
echo "NameNode UI: http://localhost:9870"
echo "ResourceManager UI: http://localhost:8088"
echo "================================================"

================
File: single_node/build.md
================
# Hadoop Docker Setup 

docker build -t hadoop-single .
docker run -d --name hadoop-single-node -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname localhost hadoop-single
docker exec -it hadoop-single-node bash


- **HDFS NameNode Web UI**: http://localhost:9870
- **YARN ResourceManager Web UI**: http://localhost:8088

================
File: single_node/check-hadoop.sh
================
#!/bin/bash
echo "=== Hadoop Services Health Check ==="
echo

# Check if SSH is running
echo "1. SSH Service Status:"
sudo service ssh status | grep -E "(Active|inactive)"
echo

# Check environment variables
echo "2. Environment Variables:"
echo "JAVA_HOME: $JAVA_HOME"
echo "HADOOP_HOME: $HADOOP_HOME"
echo "PATH contains Hadoop: $(echo $PATH | grep -o 'hadoop' | head -1)"
echo

# Check Java version
echo "3. Java Version:"
java -version 2>&1 | head -1
echo

# Check running Hadoop processes
echo "4. Running Hadoop Processes:"
jps
echo

# Check HDFS status
echo "5. HDFS Health:"
if command -v hdfs &> /dev/null; then
    $HADOOP_HOME/bin/hdfs dfsadmin -report | head -10
else
    echo "HDFS command not found - check HADOOP_HOME"
fi
echo

# Check web UIs accessibility
echo "6. Web UI Status:"
echo "NameNode Web UI: http://localhost:9870"
echo "ResourceManager Web UI: http://localhost:8088"
echo

echo "=== Health Check Complete ==="

================
File: single_node/config/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hadoop/tmp</value>
    </property>
</configuration>

================
File: single_node/config/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/hadoop/data/datanode</value>
    </property>
</configuration>

================
File: single_node/config/mapred-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

================
File: single_node/config/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

================
File: single_node/Dockerfile
================
FROM ubuntu:20.04

# Cài đặt các dependencies
RUN apt-get update && \
    apt-get install -y openjdk-8-jdk wget ssh openssh-server openssh-client rsync sudo && \
    apt-get clean

# Tạo user hadoop
RUN useradd -m -s /bin/bash hadoop && \
    echo 'hadoop:hadoop' | chpasswd && \
    adduser hadoop sudo

# Cài đặt Hadoop
USER hadoop
WORKDIR /home/hadoop

RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

# Thiết lập biến môi trường
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# Thêm environment variables vào bash profile để SSH session có thể sử dụng
RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_HOME=/home/hadoop/hadoop' >> /home/hadoop/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> /home/hadoop/.bashrc

# Copy các file cấu hình và startup script
USER root
COPY config/* /home/hadoop/hadoop/etc/hadoop/
COPY start-hadoop.sh /home/hadoop/
COPY check-hadoop.sh /home/hadoop/

# cấp quyền chạy và đổi quyền chủ sở hữu
RUN chown -R hadoop:hadoop /home/hadoop/hadoop && \
    # chmod +x /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh && \
    chmod +x /home/hadoop/start-hadoop.sh && \
    chmod +x /home/hadoop/check-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/start-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/check-hadoop.sh

# SSH configuration for root
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# SSH configuration for hadoop user
USER hadoop
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# Expose ports
EXPOSE 9870 8088 9000

# Switch to hadoop user and set working directory
USER hadoop
WORKDIR /home/hadoop

CMD ["./start-hadoop.sh"]

================
File: single_node/start-hadoop.sh
================
#!/bin/bash

# Source the Hadoop environment
# source $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Ensure environment variables are set
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export YARN_LOG_DIR=$HADOOP_HOME/logs


export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"


# Format the namenode (only if not already formatted)
if [ ! -d "/home/hadoop/hadoop/logs/hadoop" ]; then
    echo "Formatting namenode..."
    $HADOOP_HOME/bin/hdfs namenode -format -force
fi


$HADOOP_HOME/bin/hdfs --daemon start namenode
$HADOOP_HOME/bin/hdfs --daemon start datanode
$HADOOP_HOME/bin/yarn --daemon start resourcemanager
$HADOOP_HOME/bin/yarn --daemon start nodemanager

# Start Hadoop services
# echo "Starting HDFS services..."
# $HADOOP_HOME/sbin/start-dfs.sh

# echo "Starting YARN services..."
# $HADOOP_HOME/sbin/start-yarn.sh

# Show running services
echo "Hadoop services status:"
jps

# Keep container running
echo "All Hadoop services started successfully!"
echo "HDFS NameNode Web UI: http://localhost:9870"
echo "YARN ResourceManager Web UI: http://localhost:8088"
echo "Keeping container alive..."
tail -f /dev/null

================
File: stop-cluster.sh
================
#!/bin/bash
echo "Stopping Hadoop cluster..."
docker-compose down
echo "Cluster stopped."
