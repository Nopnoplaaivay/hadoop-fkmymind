This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2025-08-19T17:07:26.464Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
build.md
check-hadoop.sh
config/core-site.xml
config/hadoop-env.sh
config/hdfs-site.xml
config/mapred-site.xml
config/yarn-site.xml
docker-compose.yml
Dockerfile
start-hadoop.sh

================================================================
Repository Files
================================================================

================
File: build.md
================
# Hadoop Docker Setup Guide

## Bước 1: Build Docker Image
```bash
docker build -t hadoop-single .
```

## Bước 2: Chạy Container
```bash
docker run -d --name hadoop-single-node -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname localhost hadoop-single
```

## Bước 3: Kiểm tra Container đang chạy
```bash
docker ps
```

## Bước 4: Vào Container để kiểm tra services
```bash
docker exec -it hadoop-single-node bash
```

Trong container, kiểm tra các services:
```bash
jps  # Sẽ thấy NameNode, DataNode, ResourceManager, NodeManager
```

## Bước 5: Truy cập Web UI
- **HDFS NameNode Web UI**: http://localhost:9870
- **YARN ResourceManager Web UI**: http://localhost:8088

## Bước 6: Test HDFS (trong container)
```bash
# Tạo thư mục trong HDFS
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoop

# Upload file test
echo "Hello Hadoop from Docker!" > test.txt
hdfs dfs -put test.txt /user/hadoop/

# List files trong HDFS
hdfs dfs -ls /user/hadoop/

# Đọc file từ HDFS
hdfs dfs -cat /user/hadoop/test.txt
```

## Bước 7: Chạy MapReduce job mẫu
```bash
# Chạy Pi estimation job
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 2 100

# Chạy WordCount job
echo "hello world hello hadoop" > input.txt
hdfs dfs -mkdir /input
hdfs dfs -put input.txt /input/
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output
hdfs dfs -cat /output/part-r-00000
```

## Các lệnh quản lý Container
```bash
# Dừng container
docker stop hadoop-single-node

# Khởi động lại container
docker start hadoop-single-node

# Xóa container
docker rm hadoop-single-node

# Xem logs
docker logs hadoop-single-node

# Xem logs real-time
docker logs -f hadoop-single-node
```

## Troubleshooting

### Lỗi JAVA_HOME không được set
Container đã được cấu hình để tự động fix lỗi này.

### Lỗi SSH Connection Refused
Nếu gặp lỗi "ssh: connect to host localhost port 22: Connection refused":
```bash
# Trong container, chạy:
sudo service ssh start
```

### Environment Variables không được load khi SSH
Nếu gặp lỗi "$HADOOP_HOME" không được set sau khi SSH:
```bash
# Set lại environment variables:
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/home/hadoop/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
```

### Chỉ có ResourceManager chạy, thiếu NameNode/DataNode
```bash
# Khởi động lại HDFS services:
sudo service ssh start
$HADOOP_HOME/sbin/start-dfs.sh
jps  # Kiểm tra lại services
```

### Rebuild image nếu cần
```bash
docker stop hadoop-single-node
docker rm hadoop-single-node
docker build -t hadoop-single .
docker run -d --name hadoop-single-node -p 9870:9870 -p 8088:8088 -p 9000:9000 --hostname localhost hadoop-single
```

### Kiểm tra logs chi tiết
```bash
# Xem logs container
docker logs hadoop-single-node

# Vào container để debug
docker exec -it hadoop-single-node bash

# Kiểm tra Hadoop logs
ls -la ~/hadoop/logs/
tail -f ~/hadoop/logs/hadoop-*.log
```

================
File: check-hadoop.sh
================
#!/bin/bash
echo "=== Hadoop Services Health Check ==="
echo

# Check if SSH is running
echo "1. SSH Service Status:"
sudo service ssh status | grep -E "(Active|inactive)"
echo

# Check environment variables
echo "2. Environment Variables:"
echo "JAVA_HOME: $JAVA_HOME"
echo "HADOOP_HOME: $HADOOP_HOME"
echo "PATH contains Hadoop: $(echo $PATH | grep -o 'hadoop' | head -1)"
echo

# Check Java version
echo "3. Java Version:"
java -version 2>&1 | head -1
echo

# Check running Hadoop processes
echo "4. Running Hadoop Processes:"
jps
echo

# Check HDFS status
echo "5. HDFS Health:"
if command -v hdfs &> /dev/null; then
    $HADOOP_HOME/bin/hdfs dfsadmin -report | head -10
else
    echo "HDFS command not found - check HADOOP_HOME"
fi
echo

# Check web UIs accessibility
echo "6. Web UI Status:"
echo "NameNode Web UI: http://localhost:9870"
echo "ResourceManager Web UI: http://localhost:8088"
echo

echo "=== Health Check Complete ==="

================
File: config/core-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hadoop/tmp</value>
    </property>
</configuration>

================
File: config/hadoop-env.sh
================
#!/bin/bash

# Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Set Hadoop specific environment variables
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export YARN_LOG_DIR=$HADOOP_HOME/logs

# Set SSH options to avoid host key checking
export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"

================
File: config/hdfs-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/hadoop/data/datanode</value>
    </property>
</configuration>

================
File: config/mapred-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>

================
File: config/yarn-site.xml
================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>

================
File: docker-compose.yml
================
version: '3.8'

services:
  hadoop-single:
    build: .
    container_name: hadoop-single
    hostname: hadoop-single
    ports:
      - "9870:9870"  # HDFS NameNode Web UI
      - "8088:8088"  # YARN ResourceManager Web UI
      - "9864:9864"  # DataNode Web UI
      - "8042:8042"  # NodeManager Web UI
      - "9000:9000"  # HDFS NameNode
    volumes:
      - ./hadoop-config:/opt/hadoop/etc/hadoop
      - hadoop-data:/tmp/hadoop-root
    environment:
      - JAVA_HOME=/usr/lib/jvm/java-8-openjdk
    command: >
      bash -c "
        service ssh start &&
        hdfs namenode -format -force &&
        start-dfs.sh &&
        start-yarn.sh &&
        tail -f /dev/null
      "

volumes:
  hadoop-data:

================
File: Dockerfile
================
FROM ubuntu:20.04

# Cài đặt các dependencies
RUN apt-get update && \
    apt-get install -y openjdk-8-jdk wget ssh openssh-server openssh-client rsync sudo && \
    apt-get clean

# Tạo user hadoop
RUN useradd -m -s /bin/bash hadoop && \
    echo 'hadoop:hadoop' | chpasswd && \
    adduser hadoop sudo

# Cài đặt Hadoop
USER hadoop
WORKDIR /home/hadoop

RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

# Thiết lập biến môi trường
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/home/hadoop/hadoop
ENV PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# Thêm environment variables vào bash profile để SSH session có thể sử dụng
RUN echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_HOME=/home/hadoop/hadoop' >> /home/hadoop/.bashrc && \
    echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> /home/hadoop/.bashrc && \
    echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> /home/hadoop/.bashrc

# Copy các file cấu hình và startup script
USER root
COPY config/* /home/hadoop/hadoop/etc/hadoop/
COPY start-hadoop.sh /home/hadoop/
COPY check-hadoop.sh /home/hadoop/

# cấp quyền chạy và đổi quyền chủ sở hữu
RUN chown -R hadoop:hadoop /home/hadoop/hadoop && \
    # chmod +x /home/hadoop/hadoop/etc/hadoop/hadoop-env.sh && \
    chmod +x /home/hadoop/start-hadoop.sh && \
    chmod +x /home/hadoop/check-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/start-hadoop.sh && \
    chown hadoop:hadoop /home/hadoop/check-hadoop.sh

# SSH configuration for root
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# SSH configuration for hadoop user
USER hadoop
RUN mkdir -p ~/.ssh && \
    ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
    chmod 0600 ~/.ssh/authorized_keys && \
    echo "Host *" >> ~/.ssh/config && \
    echo "  StrictHostKeyChecking no" >> ~/.ssh/config && \
    echo "  UserKnownHostsFile=/dev/null" >> ~/.ssh/config && \
    chmod 600 ~/.ssh/config

# Expose ports
EXPOSE 9870 8088 9000

# Switch to hadoop user and set working directory
USER hadoop
WORKDIR /home/hadoop

CMD ["./start-hadoop.sh"]

================
File: start-hadoop.sh
================
#!/bin/bash

# Source the Hadoop environment
# source $HADOOP_HOME/etc/hadoop/hadoop-env.sh

# Ensure environment variables are set
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export YARN_LOG_DIR=$HADOOP_HOME/logs


export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"


# Format the namenode (only if not already formatted)
if [ ! -d "/home/hadoop/hadoop/logs/hadoop" ]; then
    echo "Formatting namenode..."
    $HADOOP_HOME/bin/hdfs namenode -format -force
fi

echo "Starting ssh services..."
sudo service ssh start

sleep 5

# Start Hadoop services
echo "Starting HDFS services..."
$HADOOP_HOME/sbin/start-dfs.sh

echo "Starting YARN services..."
$HADOOP_HOME/sbin/start-yarn.sh

# Show running services
echo "Hadoop services status:"
jps

# Keep container running
echo "All Hadoop services started successfully!"
echo "HDFS NameNode Web UI: http://localhost:9870"
echo "YARN ResourceManager Web UI: http://localhost:8088"
echo "Keeping container alive..."
tail -f /dev/null
