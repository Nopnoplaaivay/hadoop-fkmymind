FROM ubuntu:20.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    openjdk-8-jdk \
    python3 \
    python3-pip \
    wget \
    vim \
    net-tools \
    curl \
    ssh \
    sudo \
    && rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

RUN useradd -m -s /bin/bash spark && \
    echo "spark:spark" | chpasswd && \
    adduser spark sudo

USER spark
WORKDIR /home/spark

RUN wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz && \
    tar -xzf spark-3.5.0-bin-hadoop3.tgz && \
    mv spark-3.5.0-bin-hadoop3 spark && \
    rm spark-3.5.0-bin-hadoop3.tgz

RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz && \
    tar -xzf hadoop-3.3.4.tar.gz && \
    mv hadoop-3.3.4 hadoop && \
    rm hadoop-3.3.4.tar.gz

ENV SPARK_HOME=/home/spark/spark
ENV SPARK_LOG_DIR=/home/spark/spark-logs
ENV HADOOP_HOME=/home/spark/hadoop
ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin:$HADOOP_HOME/bin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

USER root
RUN pip3 install --no-cache-dir \
    pyspark==3.5.0 \
    numpy \
    pandas \
    matplotlib \
    seaborn \
    jupyter \
    notebook \
    findspark \
    pyarrow

USER spark
RUN mkdir -p /home/spark/notebooks && \
    mkdir -p /home/spark/data && \
    mkdir -p /home/spark/spark-logs && \
    mkdir -p /home/spark/hadoop-conf && \
    mkdir -p /home/spark/spark/spark-conf

RUN mkdir -p /home/spark/hadoop/etc/hadoop

EXPOSE 8080 8081 7077 4040 4041 4042 4043 8888 9999 6066

USER spark
WORKDIR /home/spark

CMD ["/bin/bash"]
