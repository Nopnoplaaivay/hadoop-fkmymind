{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865f5c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 1: Initialize PySpark ==========\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create or get SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Jupyter-Tutorial\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f571a12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 2: Test HDFS Connection ==========\n",
    "# List HDFS root directory\n",
    "import subprocess\n",
    "result = subprocess.run(['hdfs', 'dfs', '-ls', '/'], capture_output=True, text=True)\n",
    "print(\"HDFS Root Directory:\")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f516d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 3: Create Sample Dataset ==========\n",
    "# Generate sample e-commerce data\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Generate dates\n",
    "start_date = datetime(2024, 1, 1)\n",
    "dates = [start_date + timedelta(days=x) for x in range(90)]\n",
    "\n",
    "# Generate sample transactions\n",
    "transactions = []\n",
    "products = ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Watch']\n",
    "categories = ['Electronics', 'Electronics', 'Electronics', 'Audio', 'Wearables']\n",
    "customers = [f'Customer_{i}' for i in range(1, 101)]\n",
    "\n",
    "for _ in range(10000):\n",
    "    date = random.choice(dates)\n",
    "    product_idx = random.randint(0, len(products)-1)\n",
    "    transactions.append({\n",
    "        'date': date.strftime('%Y-%m-%d'),\n",
    "        'customer_id': random.choice(customers),\n",
    "        'product': products[product_idx],\n",
    "        'category': categories[product_idx],\n",
    "        'quantity': random.randint(1, 5),\n",
    "        'price': random.uniform(50, 2000),\n",
    "        'discount': random.uniform(0, 0.3)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(transactions)\n",
    "df.show(5)\n",
    "print(f\"Total records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6e0a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 4: Save to HDFS ==========\n",
    "# Save DataFrame to HDFS in different formats\n",
    "hdfs_base = \"hdfs://namenode:9000/jupyter-data\"\n",
    "\n",
    "# Parquet format (recommended)\n",
    "df.write.mode(\"overwrite\").parquet(f\"{hdfs_base}/transactions.parquet\")\n",
    "print(\"✓ Saved as Parquet\")\n",
    "\n",
    "# CSV format\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{hdfs_base}/transactions.csv\")\n",
    "print(\"✓ Saved as CSV\")\n",
    "\n",
    "# JSON format\n",
    "df.write.mode(\"overwrite\").json(f\"{hdfs_base}/transactions.json\")\n",
    "print(\"✓ Saved as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ee4e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 5: Read from HDFS ==========\n",
    "# Read back from HDFS\n",
    "df_parquet = spark.read.parquet(f\"{hdfs_base}/transactions.parquet\")\n",
    "df_csv = spark.read.option(\"header\", \"true\").csv(f\"{hdfs_base}/transactions.csv\")\n",
    "df_json = spark.read.json(f\"{hdfs_base}/transactions.json\")\n",
    "\n",
    "print(f\"Parquet records: {df_parquet.count()}\")\n",
    "print(f\"CSV records: {df_csv.count()}\")\n",
    "print(f\"JSON records: {df_json.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95113a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 6: Data Analysis with SQL ==========\n",
    "# Register as SQL table\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# SQL queries\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    category,\n",
    "    COUNT(*) as total_transactions,\n",
    "    SUM(quantity) as total_quantity,\n",
    "    ROUND(AVG(price), 2) as avg_price,\n",
    "    ROUND(SUM(price * quantity * (1 - discount)), 2) as total_revenue\n",
    "FROM transactions\n",
    "GROUP BY category\n",
    "ORDER BY total_revenue DESC\n",
    "\"\"\"\n",
    "\n",
    "category_summary = spark.sql(query1)\n",
    "category_summary.show()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "category_pd = category_summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a40f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 7: Visualization ==========\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Revenue by Category\n",
    "axes[0, 0].bar(category_pd['category'], category_pd['total_revenue'])\n",
    "axes[0, 0].set_title('Total Revenue by Category')\n",
    "axes[0, 0].set_xlabel('Category')\n",
    "axes[0, 0].set_ylabel('Revenue ($)')\n",
    "\n",
    "# Plot 2: Average Price by Category\n",
    "axes[0, 1].bar(category_pd['category'], category_pd['avg_price'], color='orange')\n",
    "axes[0, 1].set_title('Average Price by Category')\n",
    "axes[0, 1].set_xlabel('Category')\n",
    "axes[0, 1].set_ylabel('Avg Price ($)')\n",
    "\n",
    "# Plot 3: Transactions by Category\n",
    "axes[1, 0].pie(category_pd['total_transactions'], labels=category_pd['category'], autopct='%1.1f%%')\n",
    "axes[1, 0].set_title('Transaction Distribution')\n",
    "\n",
    "# Plot 4: Quantity Sold\n",
    "axes[1, 1].bar(category_pd['category'], category_pd['total_quantity'], color='green')\n",
    "axes[1, 1].set_title('Total Quantity Sold')\n",
    "axes[1, 1].set_xlabel('Category')\n",
    "axes[1, 1].set_ylabel('Quantity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa513448",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 8: Time Series Analysis ==========\n",
    "# Monthly sales trend\n",
    "monthly_sales = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(date, 'yyyy-MM') as month,\n",
    "    COUNT(*) as transactions,\n",
    "    ROUND(SUM(price * quantity * (1 - discount)), 2) as revenue\n",
    "FROM transactions\n",
    "GROUP BY DATE_FORMAT(date, 'yyyy-MM')\n",
    "ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "monthly_pd = monthly_sales.toPandas()\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(monthly_pd['month'], monthly_pd['revenue'], marker='o')\n",
    "plt.title('Monthly Revenue Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e8b55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 9: Customer Analysis ==========\n",
    "# Top customers\n",
    "top_customers = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as purchase_count,\n",
    "    ROUND(SUM(price * quantity * (1 - discount)), 2) as total_spent,\n",
    "    ROUND(AVG(price * quantity * (1 - discount)), 2) as avg_order_value\n",
    "FROM transactions\n",
    "GROUP BY customer_id\n",
    "ORDER BY total_spent DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 10 Customers by Revenue:\")\n",
    "top_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316cb4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 10: Machine Learning - Customer Segmentation ==========\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Prepare customer features\n",
    "customer_features = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as purchase_frequency,\n",
    "    AVG(price * quantity) as avg_order_value,\n",
    "    SUM(price * quantity * (1 - discount)) as total_spent,\n",
    "    AVG(discount) as avg_discount_used\n",
    "FROM transactions\n",
    "GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Prepare features for ML\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"purchase_frequency\", \"avg_order_value\", \"total_spent\", \"avg_discount_used\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "feature_df = assembler.transform(customer_features)\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(feature_df)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(feature_df)\n",
    "\n",
    "# Show cluster assignments\n",
    "print(\"Customer Segments:\")\n",
    "predictions.select(\"customer_id\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate clustering\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82ecc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ========== CELL 11: Save ML Model to HDFS ==========\n",
    "# Save model\n",
    "model_path = \"hdfs://namenode:9000/models/customer-segmentation\"\n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "loaded_model = KMeansModel.load(model_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ========== CELL 12: Clean Up ==========\n",
    "# Stop Spark session when done\n",
    "# spark.stop()\n",
    "print(\"Remember to stop Spark session when finished!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
