version: '3.8'

services:
  spark-master:
    image: spark-hadoop:latest
    container_name: spark-master
    hostname: spark-master
    init: true
    user: spark
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - HADOOP_CONF_DIR=/home/spark/hadoop-conf
    ports:
      - "8080:8080"   # Spark Master Web UI
      - "7077:7077"   # Spark Master Port
      - "4040:4040"   # Spark Application UI
      - "18080:18080" # Spark History Server
    volumes:
      - ./spark-standalone/spark-conf:/home/spark/spark/conf
      - ./spark-standalone/hadoop-conf:/home/spark/hadoop-conf
      - ./spark-standalone/notebooks:/home/spark/notebooks
      - ./spark-standalone/data:/home/spark/data
      - spark-logs:/home/spark/spark-logs
    networks:
      - spark-hadoop-network
    command: |
      bash -c "
        cp /home/spark/hadoop-conf/* /home/spark/hadoop/etc/hadoop/ 2>/dev/null || true
        /home/spark/spark/sbin/start-master.sh
        /home/spark/spark/sbin/start-history-server.sh
        tail -f /home/spark/spark-logs/*.out 2>/dev/null || tail -f /dev/null
      "

  spark-worker1:
    image: spark-hadoop:latest
    container_name: spark-worker1
    hostname: spark-worker1
    init: true
    user: spark
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8881
      - SPARK_WORKER_WEBUI_PORT=8081
      - HADOOP_CONF_DIR=/home/spark/hadoop-conf
    ports:
      - "8081:8081"  # Worker 1 Web UI
      - "4041:4041"  # Application UI
    volumes:
      - ./spark-standalone/spark-conf:/home/spark/spark/conf
      - ./spark-standalone/hadoop-conf:/home/spark/hadoop-conf
      - ./spark-standalone/data:/home/spark/data
      - spark-logs:/home/spark/spark-logs
    networks:
      - spark-hadoop-network
    command: |
      bash -c "
        sleep 10
        sudo chown -R spark:spark /home/spark/spark-logs || true
        cp /home/spark/hadoop-conf/* /home/spark/hadoop/etc/hadoop/ 2>/dev/null || true
        /home/spark/spark/sbin/start-worker.sh spark://spark-master:7077 --host spark-worker1 --webui-port 8081 --port 8881
        tail -f /home/spark/spark-logs/*.out 2>/dev/null || tail -f /dev/null
      "

  spark-worker2:
    image: spark-hadoop:latest
    container_name: spark-worker2
    hostname: spark-worker2
    init: true
    user: spark
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8882
      - SPARK_WORKER_WEBUI_PORT=8082
      - HADOOP_CONF_DIR=/home/spark/hadoop-conf
    ports:
      - "8082:8082"  # Worker 2 Web UI
      - "4042:4042"  # Application UI
    volumes:
      - ./spark-standalone/spark-conf:/home/spark/spark/conf
      - ./spark-standalone/hadoop-conf:/home/spark/hadoop-conf
      - ./spark-standalone/data:/home/spark/data
      - spark-logs:/home/spark/spark-logs
    networks:
      - spark-hadoop-network
    command: |
      bash -c "
        sleep 15
        sudo chown -R spark:spark /home/spark/spark-logs || true
        cp /home/spark/hadoop-conf/* /home/spark/hadoop/etc/hadoop/ 2>/dev/null || true
        /home/spark/spark/sbin/start-worker.sh spark://spark-master:7077 --host spark-worker2 --webui-port 8082 --port 8882
        tail -f /home/spark/spark-logs/*.out 2>/dev/null || tail -f /dev/null
      "

  jupyter-pyspark:
    image: spark-hadoop:latest
    container_name: jupyter-pyspark
    hostname: jupyter-pyspark
    init: true
    user: spark
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - HADOOP_CONF_DIR=/home/spark/hadoop-conf
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=jupyter  
      - PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port=8888 --ip=0.0.0.0 --allow-root --NotebookApp.token="" --NotebookApp.password=""'
      - SPARK_LOCAL_IP=jupyter-pyspark
      - SPARK_PUBLIC_DNS=jupyter-pyspark
    ports:
      - "8888:8888"  # Jupyter Notebook
      - "4043:4040"  # Spark Application UI
    volumes:
      - ./spark-standalone/spark-conf:/home/spark/spark/conf
      - ./spark-standalone/hadoop-conf:/home/spark/hadoop-conf
      - ./spark-standalone/notebooks:/home/spark/notebooks
      - ./spark-standalone/data:/home/spark/data
      - spark-logs:/home/spark/spark-logs
    networks:
      - spark-hadoop-network
    working_dir: /home/spark/notebooks
    command: |
      bash -c "
        cp /home/spark/hadoop-conf/* /home/spark/hadoop/etc/hadoop/ 2>/dev/null || true
        
        echo 'Waiting for Spark Master...'
        sleep 5
        echo 'Starting Jupyter Notebook...'
        jupyter notebook --no-browser --port=8888 --ip=0.0.0.0 --allow-root --NotebookApp.token='' --NotebookApp.password=''
      "

volumes:
  spark-logs:

networks:
  spark-hadoop-network:
    external: true
    name: spark-hadoop-network